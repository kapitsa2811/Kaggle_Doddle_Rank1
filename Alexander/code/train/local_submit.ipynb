{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TkAgg\n",
      "@common.py:  \n",
      "\tset random seed\n",
      "\t\tSEED=35202\n",
      "\tset cuda environment\n",
      "\t\ttorch.__version__              = 0.4.1\n",
      "\t\ttorch.version.cuda             = 9.0\n",
      "\t\ttorch.backends.cudnn.version() = 7005\n",
      "\t\tos['CUDA_VISIBLE_DEVICES']     = 0\n",
      "\t\ttorch.cuda.device_count()      = 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] =  '0' #'3,2,1,0'\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from common import *\n",
    "from data   import *\n",
    "\n",
    "\n",
    "\n",
    "##----------------------------------------\n",
    "from model_seresnext50 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- [START 2018-11-11_18-30-12] ----------------------------------------------------------------\n",
      "\n",
      "\tSEED         = 35202\n",
      "\tPROJECT_PATH = C:\\Users\\Admin\\Desktop\\doodle\\code\n",
      "\tout_test_dir = ../split/test\n",
      "\n",
      "** dataset setting **\n",
      "\n",
      "test_dataset : \n",
      "\tsplit        = <NIL>\n",
      "\tmode         = test\n",
      "\tcomplexity   = simplified\n",
      "\tlen(self.id) = 112199\n",
      "\n",
      "\n",
      "\n",
      "** net setting **\n",
      "Load imagenet pretrained SERESNEXT50\n",
      "<class 'model_seresnext50.Net'>\n",
      "\n",
      "\n",
      "\tcheckpoint = ../../output/backup/00126000_model_96color_r2.pth\n",
      "\t 0 256 512 768 1024 1280 1536 1792 2048 2304 2560 2816 3072 3328 3584 3840 4096 4352 4608 4864 5120 5376 5632 5888 6144 6400 6656 6912 7168 7424 7680 7936 8192 8448 8704 8960 9216 9472 9728 9984 10240 10496 10752 11008 11264 11520 11776 12032 12288 12544 12800 13056 13312 13568 13824 14080 14336 14592 14848 15104 15360 15616 15872 16128 16384 16640 16896 17152 17408 17664 17920 18176 18432 18688 18944 19200 19456 19712 19968 20224 20480 20736 20992 21248 21504 21760 22016 22272 22528 22784 23040 23296 23552 23808 24064 24320 24576 24832 25088 25344 25600 25856 26112 26368 26624 26880 27136 27392 27648 27904 28160 28416 28672 28928 29184 29440 29696 29952 30208 30464 30720 30976 31232 31488 31744 32000 32256 32512 32768 33024 33280 33536 33792 34048 34304 34560 34816 35072 35328 35584 35840 36096 36352 36608 36864 37120 37376 37632 37888 38144 38400 38656 38912 39168 39424 39680 39936 40192 40448 40704 40960 41216 41472 41728 41984 42240 42496 42752 43008 43264 43520 43776 44032 44288 44544 44800 45056 45312 45568 45824 46080 46336 46592 46848 47104 47360 47616 47872 48128 48384 48640 48896 49152 49408 49664 49920 50176 50432 50688 50944 51200 51456 51712 51968 52224 52480 52736 52992 53248 53504 53760 54016 54272 54528 54784 55040 55296 55552 55808 56064 56320 56576 56832 57088 57344 57600 57856 58112 58368 58624 58880 59136 59392 59648 59904 60160 60416 60672 60928 61184 61440 61696 61952 62208 62464 62720 62976 63232 63488 63744 64000 64256 64512 64768 65024 65280 65536 65792 66048 66304 66560 66816 67072 67328 67584 67840 68096 68352 68608 68864 69120 69376 69632 69888 70144 70400 70656 70912 71168 71424 71680 71936 72192 72448 72704 72960 73216 73472 73728 73984 74240 74496 74752 75008 75264 75520 75776 76032 76288 76544 76800 77056 77312 77568 77824 78080 78336 78592 78848 79104 79360 79616 79872 80128 80384 80640 80896 81152 81408 81664 81920 82176 82432 82688 82944 83200 83456 83712 83968 84224 84480 84736 84992 85248 85504 85760 86016 86272 86528 86784 87040 87296 87552 87808 88064 88320 88576 88832 89088 89344 89600 89856 90112 90368 90624 90880 91136 91392 91648 91904 92160 92416 92672 92928 93184 93440 93696 93952 94208 94464 94720 94976 95232 95488 95744 96000 96256 96512 96768 97024 97280 97536 97792 98048 98304 98560 98816 99072 99328 99584 99840 100096 100352 100608 100864 101120 101376 101632 101888 102144 102400 102656 102912 103168 103424 103680 103936 104192 104448 104704 104960 105216 105472 105728 105984 106240 106496 106752 107008 107264 107520 107776 108032 108288 108544 108800 109056 109312 109568 109824 110080 110336 110592 110848 111104 111360 111616 111872 112128 112199\n",
      "(112199, 340)\n",
      "\n",
      "NUM_CLASS 340\n",
      "(112199, 340)\n",
      "\n",
      "sucess!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def test_augment(drawing,label,index, augment):\n",
    "#     cache = Struct(data = drawing.copy(), label = label, index=index)\n",
    "\n",
    "    #<todo> ... different test-time augment ...\n",
    "\n",
    "    image = drawing_to_image_with_color_v2(drawing, 96, 96)\n",
    "    return image, label, None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##############################################################################################\n",
    "\n",
    "#generate prediction npy_file\n",
    "def make_npy_file_from_model(checkpoint, mode, split, augment, out_test_dir, npy_file):\n",
    "\n",
    "    ## setup  -----------------\n",
    "    # os.makedirs(out_test_dir +'/backup', exist_ok=True)\n",
    "    # backup_project_as_zip(PROJECT_PATH, out_dir +'/backup/code.test.%s.zip'%IDENTIFIER)\n",
    "\n",
    "    log = Logger()\n",
    "    log.open(out_test_dir +'/log.submit.txt',mode='a')\n",
    "    log.write('\\n--- [START %s] %s\\n\\n' % (IDENTIFIER, '-' * 64))\n",
    "    log.write('\\tSEED         = %u\\n' % SEED)\n",
    "    log.write('\\tPROJECT_PATH = %s\\n' % PROJECT_PATH)\n",
    "    log.write('\\tout_test_dir = %s\\n' % out_test_dir)\n",
    "    log.write('\\n')\n",
    "\n",
    "\n",
    "    ## dataset ----------------------------------------\n",
    "    log.write('** dataset setting **\\n')\n",
    "    batch_size  = 256 #256 #512\n",
    "\n",
    "    test_dataset = DoodleDataset(mode, split,\n",
    "                              lambda drawing, label, index : test_augment(drawing, label, index, augment),)\n",
    "    test_loader  = DataLoader(\n",
    "                        test_dataset,\n",
    "                        sampler     = SequentialSampler(test_dataset),\n",
    "                        batch_size  = batch_size,\n",
    "                        drop_last   = False,\n",
    "                        pin_memory  = True,\n",
    "                        collate_fn  = null_collate)\n",
    "\n",
    "    assert(len(test_dataset)>=batch_size)\n",
    "    log.write('test_dataset : \\n%s\\n'%(test_dataset))\n",
    "    log.write('\\n')\n",
    "\n",
    "\n",
    "    ## net ----------------------------------------\n",
    "    log.write('** net setting **\\n')\n",
    "    net = Net().cuda()\n",
    "\n",
    "    log.write('%s\\n\\n'%(type(net)))\n",
    "    log.write('\\n')\n",
    "\n",
    "\n",
    "\n",
    "    if 1:\n",
    "        log.write('\\tcheckpoint = %s\\n' % checkpoint)\n",
    "        net.load_state_dict(torch.load(checkpoint, map_location=lambda storage, loc: storage))\n",
    "\n",
    "\n",
    "        ####### start here ##########################\n",
    "        criterion = softmax_cross_entropy_criterion\n",
    "        test_num  = 0\n",
    "        probs    = []\n",
    "        truths   = []\n",
    "        losses   = []\n",
    "        corrects = []\n",
    "\n",
    "        net.set_mode('test')\n",
    "        for input, truth, cache in test_loader:\n",
    "            print('\\r\\t',test_num, end='', flush=True)\n",
    "            test_num += len(truth)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                input = input.cuda()\n",
    "                logit = data_parallel(net,input)\n",
    "                prob  = F.softmax(logit,1)\n",
    "                probs.append(prob.data.cpu().numpy())\n",
    "\n",
    "\n",
    "                if mode=='train': # debug only\n",
    "                    truth = truth.cuda()\n",
    "                    loss    = criterion(logit, truth, False)\n",
    "                    correct = metric(logit, truth, False)\n",
    "\n",
    "                    losses.append(loss.data.cpu().numpy())\n",
    "                    corrects.append(correct.data.cpu().numpy())\n",
    "                    truths.append(truth.data.cpu().numpy())\n",
    "\n",
    "\n",
    "        assert(test_num == len(test_loader.sampler))\n",
    "        print('\\r\\t',test_num, end='\\n', flush=True)\n",
    "        prob = np.concatenate(probs)\n",
    "\n",
    "        if mode=='train': # debug only\n",
    "            correct = np.concatenate(corrects)\n",
    "            truth   = np.concatenate(truths).astype(np.int32).reshape(-1,1)\n",
    "            loss    = np.concatenate(losses)\n",
    "            loss    = loss.mean()\n",
    "            correct = correct.mean(0)\n",
    "            top = [correct[0], correct[0]+correct[1], correct[0]+correct[1]+correct[2]]\n",
    "            precision = correct[0]/1 + correct[1]/2 + correct[2]/3\n",
    "            print('top      ', top)\n",
    "            print('precision', precision)\n",
    "            print('')\n",
    "    #-------------------------------------------\n",
    "\n",
    "\n",
    "    np.save(npy_file, np_float32_to_uint8(prob))\n",
    "    print(prob.shape)\n",
    "    log.write('\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def prob_to_csv(prob, key_id, csv_file):\n",
    "    top = np.argsort(-prob,1)[:,:3]\n",
    "    word = []\n",
    "    for (t0,t1,t2) in top:\n",
    "        word.append(\n",
    "            CLASS_NAME[t0] + ' ' + \\\n",
    "            CLASS_NAME[t1] + ' ' + \\\n",
    "            CLASS_NAME[t2]\n",
    "        )\n",
    "    df = pd.DataFrame({ 'key_id' : key_id , 'word' : word}).astype(str)\n",
    "    df.to_csv(csv_file, index=False, columns=['key_id', 'word'], compression='gzip')\n",
    "\n",
    "\n",
    "\n",
    "def npy_file_to_sbmit_csv(mode, split, npy_file, csv_file):\n",
    "    print('NUM_CLASS', NUM_CLASS)\n",
    "    complexity='simplified'\n",
    "\n",
    "    if mode=='train':\n",
    "        raise NotImplementedError\n",
    "\n",
    "    if mode=='test':\n",
    "        assert(NUM_CLASS==340)\n",
    "        global TEST_DF\n",
    "\n",
    "        if TEST_DF == []:\n",
    "            TEST_DF = pd.read_csv(DATA_DIR + '/csv/test_%s.csv'%(complexity))\n",
    "        key_id = TEST_DF['key_id'].values\n",
    "\n",
    "\n",
    "    prob = np_uint8_to_float32(np.load(npy_file))\n",
    "    print(prob.shape)\n",
    "\n",
    "    prob_to_csv(prob, key_id, csv_file)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#################################################################################################3\n",
    "\n",
    "def run_test_fold():\n",
    "\n",
    "    mode  = 'test' #'train'\n",
    "    configures =[\n",
    "         Struct(\n",
    "            split        = '<NIL>', #'valid_0', #\n",
    "            out_test_dir = '../split/test',\n",
    "            checkpoint   = '../../output/backup/00126000_model_96color_r2.pth',\n",
    "         ),\n",
    "    ]\n",
    "\n",
    "\n",
    "    for configure in configures:\n",
    "        split        = configure.split\n",
    "        out_test_dir = configure.out_test_dir\n",
    "        checkpoint   = configure.checkpoint\n",
    "        augment      = 'null'\n",
    "\n",
    "        npy_file = out_test_dir + '/%s-%s.prob.uint8.npy'%(mode,augment)\n",
    "        csv_file = out_test_dir + '/%s-%s.submit_885.csv.gz'%(mode,augment)\n",
    "\n",
    "        make_npy_file_from_model(checkpoint, mode, split, augment, out_test_dir, npy_file)\n",
    "        npy_file_to_sbmit_csv(mode, split, npy_file, csv_file)\n",
    "\n",
    "\n",
    "\n",
    "# main #################################################################\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    run_test_fold()\n",
    "\n",
    "\n",
    "    print('\\nsucess!')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../split/test/test-null.submit.csv.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-e54aa12cdb93>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../split/test/test-null.submit.csv.gz'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    760\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 762\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    763\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m    964\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 966\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    967\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    968\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1580\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1582\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1584\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas\\_libs\\parsers.c:4209)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas\\_libs\\parsers.c:7620)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\gzip.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[0;32m    161\u001b[0m             \u001b[0mmode\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m'b'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m             \u001b[0mfileobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmyfileobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'name'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../split/test/test-null.submit.csv.gz'"
     ]
    }
   ],
   "source": [
    "file = pd.read_csv('../split/test/test-null.submit.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
