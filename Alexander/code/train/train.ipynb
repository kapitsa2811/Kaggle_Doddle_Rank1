{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TkAgg\n",
      "@common.py:  \n",
      "\tset random seed\n",
      "\t\tSEED=35202\n",
      "\tset cuda environment\n",
      "\t\ttorch.__version__              = 0.4.0\n",
      "\t\ttorch.version.cuda             = 8.0.61\n",
      "\t\ttorch.backends.cudnn.version() = 7102\n",
      "\t\tos['CUDA_VISIBLE_DEVICES']     = 0\n",
      "\t\ttorch.cuda.device_count()      = 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] =  '0' #'3,2,1,0'\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from common import *\n",
    "from data   import *\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "##----------------------------------------\n",
    "from model_seresnext50 import *\n",
    "# from model_resnet34 import *\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = '25k_mixup'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=4.0, use_cuda=True):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    if use_cuda:\n",
    "        index = torch.randperm(batch_size).cuda()\n",
    "    else:\n",
    "        index = torch.randperm(batch_size)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_augment(drawing, label, index):\n",
    "#     image = drawing_to_image_with_color_v2(drawing, 96, 96)\n",
    "    seq = iaa.Sequential([\n",
    "    iaa.Crop(percent=(0.05, 0.05, \\\n",
    "                      0.05, 0.05), keep_size=True)\n",
    "    ])\n",
    "    image = drawing_to_image_with_color_aug(drawing, 96, 96, seq)\n",
    "    return image, label, None\n",
    "\n",
    "\n",
    "def train_augment(drawing, label, index):\n",
    "    up_rand = np.random.random()\n",
    "    right_rand = np.random.random()\n",
    "    percent_crop = 0.1\n",
    "    seq = iaa.Sequential([\n",
    "    iaa.Fliplr(0.5),\n",
    "    iaa.Crop(percent=(up_rand*percent_crop, right_rand*percent_crop, \\\n",
    "                      (1-up_rand)*percent_crop, (1-right_rand)*percent_crop), keep_size=True)\n",
    "    ])\n",
    "    \n",
    "    image = drawing_to_image_with_color_aug(drawing, 96, 96, seq)\n",
    "#     image = drawing_to_image_with_color_v2(drawing, 96, 96)\n",
    "    return image, label, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### training ##############################################################\n",
    "\n",
    "def do_valid( net, valid_loader, criterion ):\n",
    "\n",
    "    valid_num  = 0\n",
    "    probs    = []\n",
    "    truths   = []\n",
    "    losses   = []\n",
    "    corrects = []\n",
    "\n",
    "    for input, truth, cache in valid_loader:\n",
    "        input = input.cuda()\n",
    "        truth = truth.cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logit   = net(input)\n",
    "            prob    = F.softmax(logit,1)\n",
    "\n",
    "            loss    = criterion(logit, truth, False)\n",
    "            correct = metric(logit, truth, False)\n",
    "\n",
    "        valid_num += len(input)\n",
    "        probs.append(prob.data.cpu().numpy())\n",
    "        losses.append(loss.data.cpu().numpy())\n",
    "        corrects.append(correct.data.cpu().numpy())\n",
    "        truths.append(truth.data.cpu().numpy())\n",
    "\n",
    "\n",
    "    assert(valid_num == len(valid_loader.sampler))\n",
    "    #------------------------------------------------------\n",
    "    prob    = np.concatenate(probs)\n",
    "    correct = np.concatenate(corrects)\n",
    "    truth   = np.concatenate(truths).astype(np.int32).reshape(-1,1)\n",
    "    loss    = np.concatenate(losses)\n",
    "\n",
    "\n",
    "    #---\n",
    "    #top = np.argsort(-predict,1)[:,:3]\n",
    "\n",
    "    loss    = loss.mean()\n",
    "    correct = correct.mean(0)\n",
    "\n",
    "    top = [correct[0], correct[0]+correct[1], correct[0]+correct[1]+correct[2]]\n",
    "    precision = correct[0]/1 + correct[1]/2 + correct[2]/3\n",
    "\n",
    "    #----\n",
    "    valid_loss = np.array([\n",
    "        loss, top[0], top[2], precision\n",
    "    ])\n",
    "\n",
    "    return valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- [START 2018-11-14_19-38-25] ----------------------------------------------------------------\n",
      "\n",
      "\tSEED         = 35202\n",
      "\tPROJECT_PATH = /data/ml-arsenal/projects/Doodle/Venn/code\n",
      "\t__file__     = 25k_mixup\n",
      "\tout_dir      = ../../output\n",
      "\n",
      "\t<additional comments>\n",
      "\t  ... xxx baseline  ... \n",
      "\n",
      "** dataset setting **\n",
      "\t load df   :  339/340                   zigzag   0 min 26 sec\n",
      "\t load split:  339/340                   zigzag   0 min 27 sec\n",
      "\n",
      "\t load df   :  339/340                   zigzag   0 min 00 sec\n",
      "\t load split:  339/340                   zigzag   0 min 00 sec\n",
      "\n",
      "batch_size = 320\n",
      "train_dataset : \n",
      "\tsplit        = train_0\n",
      "\tmode         = train\n",
      "\tcomplexity   = simplified\n",
      "\tlen(self.id) = 8500000\n",
      "\n",
      "\n",
      "valid_dataset : \n",
      "\tsplit        = valid_0\n",
      "\tmode         = valid\n",
      "\tcomplexity   = simplified\n",
      "\tlen(self.id) = 27200\n",
      "\n",
      "\n",
      "\n",
      "** net setting **\n",
      "Load imagenet pretrained SERESNEXT50\n"
     ]
    }
   ],
   "source": [
    "fold    = 0\n",
    "out_dir = \\\n",
    "    '../../output'\n",
    "initial_checkpoint = None #\\\n",
    "        #'../../output/backup/873_crop.pth'\n",
    "\n",
    "pretrain_file = None\n",
    "\n",
    "batch_size = 256+64\n",
    "epoch = 20\n",
    "num_iters   = epoch * 340 * 25000 // batch_size\n",
    "\n",
    "#     schduler  = NullScheduler(lr=0.01)\n",
    "schduler = DecayScheduler(base_lr=0.01, decay=0.1, step=num_iters/2)\n",
    "iter_save_interval = 2000\n",
    "criterion          = softmax_cross_entropy_criterion\n",
    "\n",
    "\n",
    "## setup  -----------------------------------------------------------------------------\n",
    "os.makedirs(out_dir +'/checkpoint', exist_ok=True)\n",
    "os.makedirs(out_dir +'/train', exist_ok=True)\n",
    "os.makedirs(out_dir +'/backup', exist_ok=True)\n",
    "#     backup_project_as_zip(PROJECT_PATH, out_dir +'/backup/code.train.%s.zip'%IDENTIFIER)\n",
    "\n",
    "log = Logger()\n",
    "log.open(out_dir+'/log.train_r50_add_crop.txt',mode='a')\n",
    "log.write('\\n--- [START %s] %s\\n\\n' % (IDENTIFIER, '-' * 64))\n",
    "log.write('\\tSEED         = %u\\n' % SEED)\n",
    "log.write('\\tPROJECT_PATH = %s\\n' % PROJECT_PATH)\n",
    "log.write('\\t__file__     = %s\\n' % FILE_NAME)\n",
    "log.write('\\tout_dir      = %s\\n' % out_dir)\n",
    "log.write('\\n')\n",
    "log.write('\\t<additional comments>\\n')\n",
    "log.write('\\t  ... xxx baseline  ... \\n')\n",
    "log.write('\\n')\n",
    "\n",
    "\n",
    "## dataset ----------------------------------------\n",
    "log.write('** dataset setting **\\n')\n",
    "\n",
    "train_dataset = DoodleDataset('train', 'train_0', train_augment)\n",
    "train_loader  = DataLoader(\n",
    "                    train_dataset,\n",
    "                    #sampler     = FixLengthRandomSamplerWithProbability(train_dataset, probability),\n",
    "                    #sampler     = FixLengthRandomSampler(train_dataset),\n",
    "                    #sampler     = ConstantSampler(train_dataset,[31]*batch_size*100),\n",
    "                    sampler     = RandomSampler(train_dataset),\n",
    "                    batch_size  = batch_size,\n",
    "                    num_workers = 8,\n",
    "                    drop_last   = True,\n",
    "                    pin_memory  = True,\n",
    "                    collate_fn  = null_collate)\n",
    "\n",
    "valid_dataset = DoodleDataset('valid', 'valid_0',  valid_augment)\n",
    "valid_loader  = DataLoader(\n",
    "                    valid_dataset,\n",
    "                    #sampler     = SequentialSampler(valid_dataset),\n",
    "                    sampler     = RandomSampler(valid_dataset),\n",
    "                    batch_size  = batch_size,\n",
    "                    num_workers = 8,\n",
    "                    drop_last   = False,\n",
    "                    pin_memory  = True,\n",
    "                    collate_fn  = null_collate)\n",
    "\n",
    "\n",
    "assert(len(train_dataset)>=batch_size)\n",
    "log.write('batch_size = %d\\n'%(batch_size))\n",
    "log.write('train_dataset : \\n%s\\n'%(train_dataset))\n",
    "log.write('valid_dataset : \\n%s\\n'%(valid_dataset))\n",
    "log.write('\\n')\n",
    "\n",
    "## net ----------------------------------------\n",
    "log.write('** net setting **\\n')\n",
    "net = Net().cuda()\n",
    "\n",
    "if initial_checkpoint is not None:\n",
    "    log.write('\\tinitial_checkpoint = %s\\n' % initial_checkpoint)\n",
    "    net.load_state_dict(torch.load(initial_checkpoint, map_location=lambda storage, loc: storage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'model_seresnext50.Net'>\n",
      "criterion=<function softmax_cross_entropy_criterion at 0x7fe9b1629048>\n",
      "\n",
      "schduler\n",
      "  (Exp) Decay Learning Rates\n",
      "base_lr=0.010, decay=0.100, step=265625.000\n",
      "\n",
      "** start training here! **\n",
      "                    |------------ VALID -------------|-------- TRAIN/BATCH ----------|         \n",
      "rate   iter  epoch  | loss   acc-1  acc-3   lb       | loss   acc-1  acc-3   lb      |  time   \n",
      "----------------------------------------------------------------------------------------------------\n",
      "0.0100   0.5    0.0 | 2.026  0.536  0.747  (0.631)   | 3.083  0.214  0.325  (0.262)  |  0 hr 05 min\n",
      "0.0100   1.0    0.0 | 1.441  0.653  0.837  (0.736)   | 2.364  0.244  0.341  (0.287)  |  0 hr 10 min\n",
      "0.0100   1.5    0.1 | 1.294  0.688  0.858  (0.765)   | 2.396  0.264  0.346  (0.301)  |  0 hr 16 min\n",
      "0.0100   2.0    0.1 | 1.194  0.708  0.873  (0.783)*  | 2.064  0.380  0.489  (0.429)  |  0 hr 21 min\n",
      "0.0100   2.5    0.1 | 1.145  0.721  0.879  (0.793)   | 2.096  0.298  0.383  (0.336)  |  0 hr 26 min\n",
      "0.0100   3.0    0.1 | 1.140  0.729  0.882  (0.799)   | 2.077  0.368  0.472  (0.415)  |  0 hr 32 min\n",
      "0.0100   3.5    0.1 | 1.102  0.740  0.889  (0.808)   | 2.165  0.293  0.371  (0.328)  |  0 hr 37 min\n",
      "0.0100   4.0    0.2 | 1.042  0.741  0.891  (0.810)*  | 1.614  0.273  0.337  (0.302)  |  0 hr 42 min\n",
      "0.0100   4.5    0.2 | 1.008  0.747  0.895  (0.815)   | 2.069  0.371  0.474  (0.417)  |  0 hr 47 min\n",
      "0.0100   5.0    0.2 | 1.035  0.750  0.896  (0.817)   | 1.921  0.322  0.403  (0.358)  |  0 hr 53 min\n",
      "0.0100   5.5    0.2 | 0.987  0.754  0.898  (0.820)   | 1.670  0.402  0.487  (0.440)  |  0 hr 58 min\n",
      "0.0100   6.0    0.2 | 1.028  0.753  0.898  (0.820)*  | 1.724  0.399  0.490  (0.440)  |  1 hr 03 min\n",
      "0.0100   6.2    0.2 | 1.028  0.753  0.898  (0.820)   | 3.572  0.009  0.025  (0.016)  |  1 hr 05 min"
     ]
    }
   ],
   "source": [
    "log.write('%s\\n'%(type(net)))\n",
    "log.write('criterion=%s\\n'%criterion)\n",
    "log.write('\\n')\n",
    "\n",
    "\n",
    "## optimiser ----------------------------------\n",
    "if 0: ##freeze\n",
    "    for p in net.resnet.parameters(): p.requires_grad = False\n",
    "    for p in net.encoder1.parameters(): p.requires_grad = False\n",
    "    for p in net.encoder2.parameters(): p.requires_grad = False\n",
    "    for p in net.encoder3.parameters(): p.requires_grad = False\n",
    "    for p in net.encoder4.parameters(): p.requires_grad = False\n",
    "    pass\n",
    "\n",
    "#net.set_mode('train',is_freeze_bn=True)\n",
    "#-----------------------------------------------\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(filter(lambda p: p.requires_grad, net.parameters()),\n",
    "                      lr=schduler.get_rate(0), momentum=0.9, weight_decay=0.0001)\n",
    "\n",
    "\n",
    "iter_smooth = 20\n",
    "iter_log    = 50\n",
    "iter_valid  = 500\n",
    "iter_save   = [0, num_iters-1]\\\n",
    "               + list(range(0, num_iters, iter_save_interval))#1*1000\n",
    "\n",
    "start_iter = 0\n",
    "start_epoch= 0\n",
    "rate       = 0\n",
    "if initial_checkpoint is not None:\n",
    "#     initial_optimizer = initial_checkpoint.replace('_model.pth','_optimizer.pth')\n",
    "#     checkpoint  = torch.load(initial_optimizer)\n",
    "#     start_iter  = checkpoint['iter' ]\n",
    "#     start_epoch = checkpoint['epoch']\n",
    "\n",
    "    #rate = get_learning_rate(optimizer)  #load all except learning rate\n",
    "    #optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    #adjust_learning_rate(optimizer, rate)\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "log.write('schduler\\n  %s\\n'%(schduler))\n",
    "log.write('\\n')\n",
    "\n",
    "## start training here! ##############################################\n",
    "log.write('** start training here! **\\n')\n",
    "log.write('                    |------------ VALID -------------|-------- TRAIN/BATCH ----------|         \\n')\n",
    "log.write('rate   iter  epoch  | loss   acc-1  acc-3   lb       | loss   acc-1  acc-3   lb      |  time   \\n')\n",
    "log.write('----------------------------------------------------------------------------------------------------\\n')\n",
    "\n",
    "\n",
    "train_loss   = np.zeros(6,np.float32)\n",
    "valid_loss   = np.zeros(6,np.float32)\n",
    "batch_loss   = np.zeros(6,np.float32)\n",
    "iter = 0\n",
    "i    = 0\n",
    "last_max_lb   = -1\n",
    "\n",
    "\n",
    "start = timer()\n",
    "while  iter<num_iters:\n",
    "    sum_train_loss = np.zeros(6,np.float32)\n",
    "    sum = 0\n",
    "\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    for input, truth, cache in train_loader:\n",
    "\n",
    "        len_train_dataset = len(train_dataset)\n",
    "        batch_size = len(cache)\n",
    "        iter = i + start_iter\n",
    "        epoch = (iter-start_iter)*batch_size/len_train_dataset + start_epoch\n",
    "        num_samples = epoch*len_train_dataset\n",
    "\n",
    "\n",
    "        if (iter % iter_valid==0) and (iter!=0):\n",
    "            net.set_mode('valid')\n",
    "            valid_loss = do_valid(net, valid_loader, criterion)\n",
    "            net.set_mode('train')\n",
    "\n",
    "            ##--------\n",
    "            # lb    = valid_loss[7]\n",
    "            # loss  = valid_loss[0] + valid_loss[4]\n",
    "            # last_max_lb = max(last_max_lb,lb)\n",
    "            # if last_max_lb-lb<0.005:\n",
    "            #     iter_save += [iter,]\n",
    "            # if loss-last_min_loss<0.005:\n",
    "            #     iter_save += [iter,]\n",
    "\n",
    "            asterisk = '*' if iter in iter_save else ' '\n",
    "            ##--------\n",
    "\n",
    "            print('\\r',end='',flush=True)\n",
    "            log.write('%0.4f %5.1f %6.1f | %0.3f  %0.3f  %0.3f  (%0.3f)%s  | %0.3f  %0.3f  %0.3f  (%0.3f)  | %s' % (\\\n",
    "                     rate, iter/1000, epoch,\n",
    "                     valid_loss[0], valid_loss[1], valid_loss[2], valid_loss[3],asterisk,\n",
    "                     train_loss[0], train_loss[1], train_loss[2], train_loss[3],\n",
    "                     time_to_str((timer() - start),'min'))\n",
    "            )\n",
    "            log.write('\\n')\n",
    "            time.sleep(0.01)\n",
    "\n",
    "        #if 0:\n",
    "        if iter in iter_save:\n",
    "            torch.save(net.state_dict(),out_dir +'/checkpoint/%08d_model.pth'%(iter))\n",
    "            torch.save({\n",
    "                #'optimizer': optimizer.state_dict(),\n",
    "                'iter'     : iter,\n",
    "                'epoch'    : epoch,\n",
    "            }, out_dir +'/checkpoint/%08d_optimizer.pth'%(iter))\n",
    "            pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # learning rate schduler -------------\n",
    "        lr = schduler.get_rate(iter)\n",
    "        if lr<0 : break\n",
    "        adjust_learning_rate(optimizer, lr)\n",
    "        rate = get_learning_rate(optimizer)\n",
    "\n",
    "\n",
    "\n",
    "        # one iteration update  -------------\n",
    "        #net.set_mode('train',is_freeze_bn=True)\n",
    "        net.set_mode('train')\n",
    "        #input = input.cuda()\n",
    "        #truth = truth.cuda()\n",
    "        inputs, targets_a, targets_b, lam = mixup_data(input, truth, 0.2, True)\n",
    "        \n",
    "        logit = data_parallel(net,inputs.cuda())#net(input)#data_parallel(net,input) #net(input)\n",
    "        #del input\n",
    "        targets_a = targets_a.cuda() \n",
    "        loss = mixup_criterion(criterion, logit, targets_a, targets_b.cuda(), lam)\n",
    "        precision, top = metric(logit, targets_a)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "        del logit, targets_a, targets_b\n",
    "        #gc.collect()\n",
    "        \n",
    "        #precision, top = metric(logit, targets_a)\n",
    "        \n",
    "        #with torch.no_grad():\n",
    "        #    input = input.cuda()\n",
    "        #    truth = truth.cuda()\n",
    "        #    logit = data_parallel(net, input)\n",
    "        #    #loss  = criterion(logit, truth)\n",
    "        #    precision, top = metric(logit, truth)\n",
    "\n",
    "\n",
    "        #loss.backward()\n",
    "        #optimizer.step()\n",
    "        #optimizer.zero_grad()\n",
    "        #torch.nn.utils.clip_grad_norm(net.parameters(), 1)\n",
    "\n",
    "\n",
    "        # print statistics  ------------\n",
    "        batch_loss[:4] = np.array(( loss.item(), top[0].item(), top[2].item(), precision.item(),))\n",
    "        sum_train_loss += batch_loss\n",
    "        sum += 1\n",
    "        if iter%iter_smooth == 0:\n",
    "            train_loss = sum_train_loss/sum\n",
    "            sum_train_loss = np.zeros(6,np.float32)\n",
    "            sum = 0\n",
    "\n",
    "\n",
    "        print('\\r',end='',flush=True)\n",
    "        print('%0.4f %5.1f %6.1f | %0.3f  %0.3f  %0.3f  (%0.3f)%s  | %0.3f  %0.3f  %0.3f  (%0.3f)  | %s' % (\\\n",
    "                     rate, iter/1000, epoch,\n",
    "                     valid_loss[0], valid_loss[1], valid_loss[2], valid_loss[3],' ',\n",
    "                     batch_loss[0], batch_loss[1], batch_loss[2], batch_loss[3],\n",
    "                     time_to_str((timer() - start),'min'))\n",
    "        , end='',flush=True)\n",
    "        i=i+1\n",
    "\n",
    "\n",
    "\n",
    "    pass  #-- end of one data loader --\n",
    "pass #-- end of all iterations --\n",
    "\n",
    "\n",
    "if 1: #save last\n",
    "    torch.save(net.state_dict(),out_dir +'/checkpoint/%d_model.pth'%(i))\n",
    "    torch.save({\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'iter'     : i,\n",
    "        'epoch'    : epoch,\n",
    "    }, out_dir +'/checkpoint/%d_optimizer.pth'%(i))\n",
    "\n",
    "log.write('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Protein",
   "language": "python",
   "name": "protein"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
